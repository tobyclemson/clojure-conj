<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 3.0.2 (195746)"/><meta name="keywords" content="clojure, conference, day3"/><meta name="created" content="2011-11-12 12:39:15 -0600"/><meta name="updated" content="2011-11-16 20:17:38 -0600"/><title>"Faster, Baby: A Performance Engineering Experience Report", Craig Andera</title></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">
<b>Background</b>
<ul>
<li>About me:
<ul>
<li>Work for http://thinkrelevance.com</li>
<li>Writes software</li>
<li>[github]</li>
<li>[twitter]</li>
</ul>
</li>
<li>How do you approach performance engineering?</li>
<li>Model based optimisation
<ul>
<li>A way to look at your system</li>
<li>How to make improvements in performance the right way</li>
</ul>
</li>
<li>Optimisation loop</li>
<li>How the Clojure system we worked on benefitted from this approach</li>
<li>Practitioner - this is just stuff that has worked for me</li>
</ul>
<b>Model Based Optimisation</b>
<ul>
<li>What's wrong with this picture?
<ul>
<li>Near ship time, someone says, "Time to make it faster!"</li>
<li>You go to your users to get performance requirements</li>
<li>They make up a number</li>
<li>You make the system match that number</li>
</ul>
</li>
<li>Here's what
<ul>
<li>Done late</li>
<li>Done haphazardly</li>
<li>Done in the absence of an adequate model</li>
</ul>
</li>
<li>Performance is not a scalar quantity
<ul>
<li>"How fast is the system?" rarely has an answer</li>
<li>Your system probably doesn't do just one thing</li>
<li>The world is not deterministic!</li>
<li>Need a <i>stochastic</i> model of you systems's performance
<ul>
<li>involving randomness</li>
</ul>
</li>
</ul>
</li>
<li>Models!
<ul>
<li>Useful
<ul>
<li>What is the 99% case for response time when I have 10 users?
<ul>
<li>stochastic</li>
<li>average case</li>
</ul>
</li>
<li>What will the system do if I get mentioned on HackerNews?
<ul>
<li>load testing</li>
</ul>
</li>
<li>How many servers will I need a year from now?
<ul>
<li>capacity planning</li>
</ul>
</li>
</ul>
</li>
<li>Wrong!
<ul>
<li>Be aware of <i>assumptions</i> in the model
<ul>
<li>Testing versus production</li>
<li>Small versus large data</li>
<li>Load balances or not?</li>
<li>etc.</li>
</ul>
</li>
<li>This is not an excuse for not doing this</li>
<li>How wrong do you have to be before it stops being useful?</li>
<li>"Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful?" - George E. P. Box, Empirical Model-Building</li>
</ul>
</li>
</ul>
</li>
<li>One useful model
<ul>
<li>latency "distribution" versus throughput <i>for a given transaction mix</i>
<ul>
<li>don't focus on just one thing, mix up possible transactions to be representational</li>
</ul>
</li>
<li>plus whatever other constraints you have
<ul>
<li>for instance cache expiry times</li>
</ul>
</li>
</ul>
</li>
<li>Use load testing
<ul>
<li>1 users, then 2 users, then 3 users etc.</li>
<li>graph throughput, requests against time?</li>
<li>this doesn't give you the latency though...background time or wait time in each request</li>
<li>questions we can answer with this model:
<ul>
<li>Can we meet our customers expectations?</li>
<li>How much capacity do we need to handle spikes?</li>
<li>What is it going to cost us to run this system?</li>
</ul>
</li>
</ul>
</li>
</ul>
<b>The Optimisation Loop</b>
<ul>
<li>Benchmark -> analyse -> recommend -> optimise -> loop -break-> done!
<ul>
<li>Benchmark
<ul>
<li>Do
<ul>
<li>measure the parameters of you model</li>
<li>remember that transaction mix is critical!</li>
<li>understand what you are measuring</li>
</ul>
</li>
<li>Don't
<ul>
<li>mistake # of threads for load</li>
<li>forget to look for errors</li>
<li>keep going if you're done</li>
<li>wait to start</li>
</ul>
</li>
<li>Tools
<ul>
<li>load generators: jmeter, ab, httperf</li>
<li>analysis: excel, Clojure!</li>
</ul>
</li>
</ul>
</li>
<li>Analyse
<ul>
<li>Identify the biggest factor in the perf of your system...empirically!</li>
<li>Generally this is done via profiling</li>
<li>And lots of hard thinking</li>
<li>Be aware of transaction mix</li>
</ul>
</li>
<li>Recommend
<ul>
<li>You've found the one slowest thing</li>
<li>Often easy in the early rounds</li>
<li>This will get harder the longer you go</li>
<li>Now figure out what to do about it</li>
<li>Use the data!</li>
<li>The best recommendations is "We're done. Let's stop."</li>
</ul>
</li>
<li>Optimise<br/>
<ul>
<li>Fix the one slowest thing</li>
<li>You may find redesign is necessary</li>
<li>This is why you want to start early</li>
</ul>
</li>
</ul>
</li>
<li>"Any problem in computer science can be solved with another layer of indirection", ...</li>
<li>"Except for the problem of too many layers of indirection", ...</li>
</ul>
<b>Real World Example</b>
<ul>
<li>Where we started
<ul>
<li>About 75 req/s peak</li>
<li>About 25 ms avg latency</li>
</ul>
</li>
<li>What we did
<ul>
<li>Prepped customer</li>
<li>Two-card iterative approach
<ul>
<li>One card is a combination of benchmarking and analysis
<ul>
<li>Output being enough data to come up with the recommendation card</li>
</ul>
</li>
<li>Other card is a recommendation and optimisation
<ul>
<li>Output being the work involved in the optimisation and to come up with the next benchmarking card</li>
</ul>
</li>
</ul>
</li>
<li>Several rounds of optimisation</li>
<li>Over about a month</li>
</ul>
</li>
<li>Where we got to
<ul>
<li>About 1700 req/s peak throughput</li>
<li>&lt; 10 ms latency @ 1500 req/s @ 99% confidence</li>
<li>A very happy customer</li>
</ul>
</li>
<li>Iterative approach helped them get to where they had wanted to be for 2 years in 1 month</li>
<li>Stuff that happened
<ul>
<li>Found out that logging was one of the bottlenecks
<ul>
<li>~80% of time!<br/></li>
<li>How?
<ul>
<li>Wrote a wrapper around the log library</li>
<li>This allowed the log library to be called from Java</li>
<li>This allowed the log library to be valled from Clojure</li>
</ul>
</li>
</ul>
</li>
<li>Found out a one-character change in a config file made a huge difference</li>
<li>Uncovered an issue with huge volume of log data
<ul>
<li>2 Gbs an hour!Â </li>
<li>Client wanted to keep all of that! Where!?</li>
</ul>
</li>
<li>Later, had to redesign logging infrastructure again</li>
<li>Never tested load-balanced</li>
<li>Got stress test basically for free</li>
<li>Used automated load testing, tests ran overnight, emailed us in the morning</li>
</ul>
</li>
</ul>
<b>Conclusion</b>
<ul>
<li>Be deliberate</li>
<li>Think about your model and define it</li>
<li>Be stochastic</li>
<li>Recognise that the model is wrong</li>
<li>If performance optimisations are needed use the optimisation loop: measure : adjust : measure</li>
</ul>
<b>Questions</b>
<ul>
<li>How do you verify that your benchmarks are testing what really matters?
<ul>
<li>What really matters is up to you</li>
<li>Rephrase: How do you tell whether your model is actually going to reflect what happens when you go live?</li>
<li>Build a model, work with the people who own the environments, ask if it's what they wanted</li>
<li>Use metrics, match up against model</li>
</ul>
</li>
<li>Is there anything specific about Clojure that you learnt in this process?
<ul>
<li>Clojure wasn't a performance bottleneck!</li>
<li>Nothing about Clojure itself limited us</li>
<li>Mostly database caching and logging, syslog4j</li>
<li>Choose the right data structure for the job, linear access -> random access made a big difference in one case</li>
</ul>
</li>
<li>Would you recommend building in iteration boundaries for optimisation or making it an organic part of the process?
<ul>
<li>The latter, make it a fundamental part of the development process</li>
</ul>
</li>
</ul>
</body></html>